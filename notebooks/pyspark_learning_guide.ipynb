{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac842ded",
   "metadata": {},
   "source": [
    "# PySpark Complete Learning Guide - Beginner to Advanced\n",
    "\n",
    "This notebook contains a comprehensive collection of PySpark programs organized from beginner to advanced level. Each section includes theoretical concepts, practical examples, and interview-focused problems.\n",
    "\n",
    "## Table of Contents\n",
    "1. **Environment Setup & Basic Concepts**\n",
    "2. **RDD Operations (Foundation)**\n",
    "3. **DataFrame Basics**\n",
    "4. **Data Loading & File Operations**\n",
    "5. **Data Transformations & Actions**\n",
    "6. **SQL Operations**\n",
    "7. **Joins & Advanced Operations**\n",
    "8. **Window Functions**\n",
    "9. **UDFs (User Defined Functions)**\n",
    "10. **Performance Optimization**\n",
    "11. **Streaming (Advanced)**\n",
    "12. **Interview Questions & Solutions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa0bbf3",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Basic Concepts\n",
    "\n",
    "### What is PySpark?\n",
    "- Python API for Apache Spark\n",
    "- Distributed computing framework\n",
    "- Handles big data processing across clusters\n",
    "- Supports SQL, streaming, machine learning, and graph processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859682d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PySpark (run this if not already installed)\n",
    "# !pip install pyspark\n",
    "\n",
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "\n",
    "# Create SparkSession - Entry point for all Spark functionality\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark Learning\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to reduce verbose output\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"SparkSession created successfully!\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Python Version: {spark.sparkContext.pythonVer}\")\n",
    "\n",
    "# Basic Spark Context information\n",
    "sc = spark.sparkContext\n",
    "print(f\"Application Name: {sc.appName}\")\n",
    "print(f\"Master: {sc.master}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b40a064",
   "metadata": {},
   "source": [
    "## 2. RDD Operations (Foundation)\n",
    "\n",
    "**RDD (Resilient Distributed Dataset)** is the fundamental data structure of Spark. Understanding RDDs is crucial for interviews.\n",
    "\n",
    "### Key Concepts:\n",
    "- **Immutable**: Once created, cannot be changed\n",
    "- **Distributed**: Spread across multiple nodes\n",
    "- **Fault-tolerant**: Can recover from node failures\n",
    "- **Lazy Evaluation**: Transformations are not executed until an action is called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e01af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGINNER: Creating RDDs and Basic Operations\n",
    "\n",
    "# 1. Creating RDDs from collections\n",
    "numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "rdd_numbers = sc.parallelize(numbers)\n",
    "\n",
    "# 2. Basic transformations (lazy operations)\n",
    "squared_rdd = rdd_numbers.map(lambda x: x ** 2)\n",
    "even_rdd = rdd_numbers.filter(lambda x: x % 2 == 0)\n",
    "doubled_rdd = rdd_numbers.map(lambda x: x * 2)\n",
    "\n",
    "# 3. Basic actions (trigger execution)\n",
    "print(\"Original numbers:\", rdd_numbers.collect())\n",
    "print(\"Squared numbers:\", squared_rdd.collect())\n",
    "print(\"Even numbers:\", even_rdd.collect())\n",
    "print(\"First 3 numbers:\", rdd_numbers.take(3))\n",
    "print(\"Count:\", rdd_numbers.count())\n",
    "print(\"Sum:\", rdd_numbers.reduce(lambda x, y: x + y))\n",
    "\n",
    "# 4. flatMap example\n",
    "words_list = [[\"hello\", \"world\"], [\"spark\", \"python\"], [\"big\", \"data\"]]\n",
    "words_rdd = sc.parallelize(words_list)\n",
    "flattened_rdd = words_rdd.flatMap(lambda x: x)\n",
    "print(\"Original nested list:\", words_rdd.collect())\n",
    "print(\"Flattened list:\", flattened_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0f7ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTERMEDIATE: Key-Value RDD Operations (Important for interviews)\n",
    "\n",
    "# 1. Creating key-value pairs\n",
    "students_data = [(\"Alice\", 85), (\"Bob\", 90), (\"Alice\", 95), (\"Charlie\", 78), (\"Bob\", 88)]\n",
    "students_rdd = sc.parallelize(students_data)\n",
    "\n",
    "# 2. reduceByKey - combine values for the same key\n",
    "total_scores = students_rdd.reduceByKey(lambda x, y: x + y)\n",
    "print(\"Total scores by student:\", total_scores.collect())\n",
    "\n",
    "# 3. groupByKey - group all values for each key\n",
    "grouped_scores = students_rdd.groupByKey().mapValues(list)\n",
    "print(\"All scores by student:\", grouped_scores.collect())\n",
    "\n",
    "# 4. mapValues - transform only values\n",
    "students_with_grade = students_rdd.mapValues(lambda score: \"A\" if score >= 90 else \"B\" if score >= 80 else \"C\")\n",
    "print(\"Students with grades:\", students_with_grade.collect())\n",
    "\n",
    "# 5. keys() and values()\n",
    "print(\"All student names:\", students_rdd.keys().distinct().collect())\n",
    "print(\"All scores:\", students_rdd.values().collect())\n",
    "\n",
    "# 6. sortByKey\n",
    "sorted_students = students_rdd.sortByKey(ascending=True)\n",
    "print(\"Sorted by name:\", sorted_students.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6cf19b",
   "metadata": {},
   "source": [
    "## 3. DataFrame Basics\n",
    "\n",
    "**DataFrames** are the most commonly used abstraction in PySpark. They provide:\n",
    "- Higher-level API than RDDs\n",
    "- Better performance optimization\n",
    "- Schema enforcement\n",
    "- SQL-like operations\n",
    "\n",
    "### Key Interview Points:\n",
    "- DataFrames are built on top of RDDs\n",
    "- They use Catalyst optimizer for query optimization\n",
    "- Schema is known at compile time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32a6090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGINNER: Creating DataFrames and Basic Operations\n",
    "\n",
    "# 1. Creating DataFrames from various sources\n",
    "employee_data = [\n",
    "    (1, \"John\", \"Engineering\", 75000, 28),\n",
    "    (2, \"Sarah\", \"Marketing\", 65000, 32),\n",
    "    (3, \"Mike\", \"Engineering\", 80000, 35),\n",
    "    (4, \"Lisa\", \"HR\", 60000, 29),\n",
    "    (5, \"David\", \"Engineering\", 85000, 31),\n",
    "    (6, \"Emma\", \"Marketing\", 70000, 27)\n",
    "]\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"department\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(employee_data, schema)\n",
    "\n",
    "# 2. Basic DataFrame operations\n",
    "print(\"=== Basic DataFrame Info ===\")\n",
    "df.printSchema()\n",
    "print(f\"Number of rows: {df.count()}\")\n",
    "print(f\"Number of columns: {len(df.columns)}\")\n",
    "print(\"Column names:\", df.columns)\n",
    "\n",
    "# 3. Display data\n",
    "print(\"\\n=== Show DataFrame ===\")\n",
    "df.show()\n",
    "\n",
    "# 4. Basic statistics\n",
    "print(\"\\n=== Basic Statistics ===\")\n",
    "df.describe().show()\n",
    "\n",
    "# 5. Select specific columns\n",
    "print(\"\\n=== Select Columns ===\")\n",
    "df.select(\"name\", \"department\", \"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90136448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGINNER-INTERMEDIATE: DataFrame Filtering, Transformations & Aggregations\n",
    "\n",
    "# 1. Filtering operations\n",
    "print(\"=== Filtering Examples ===\")\n",
    "\n",
    "# Filter by single condition\n",
    "high_salary_df = df.filter(df.salary > 70000)\n",
    "print(\"Employees with salary > 70000:\")\n",
    "high_salary_df.show()\n",
    "\n",
    "# Filter by multiple conditions\n",
    "engineering_high_salary = df.filter((df.department == \"Engineering\") & (df.salary > 75000))\n",
    "print(\"Engineering employees with salary > 75000:\")\n",
    "engineering_high_salary.show()\n",
    "\n",
    "# 2. Column operations\n",
    "print(\"\\n=== Column Operations ===\")\n",
    "\n",
    "# Add new column\n",
    "df_with_bonus = df.withColumn(\"bonus\", df.salary * 0.1)\n",
    "df_with_bonus.select(\"name\", \"salary\", \"bonus\").show()\n",
    "\n",
    "# Rename column\n",
    "df_renamed = df.withColumnRenamed(\"name\", \"employee_name\")\n",
    "df_renamed.select(\"employee_name\", \"department\").show()\n",
    "\n",
    "# 3. Sorting\n",
    "print(\"\\n=== Sorting Examples ===\")\n",
    "df.orderBy(\"salary\", ascending=False).show()\n",
    "\n",
    "# 4. Aggregations\n",
    "print(\"\\n=== Aggregation Examples ===\")\n",
    "from pyspark.sql.functions import avg, sum, count, max, min\n",
    "\n",
    "# Basic aggregations\n",
    "print(\"Average salary:\", df.agg(avg(\"salary\")).collect()[0][0])\n",
    "print(\"Total salary budget:\", df.agg(sum(\"salary\")).collect()[0][0])\n",
    "\n",
    "# GroupBy operations\n",
    "dept_stats = df.groupBy(\"department\").agg(\n",
    "    count(\"*\").alias(\"employee_count\"),\n",
    "    avg(\"salary\").alias(\"avg_salary\"),\n",
    "    max(\"salary\").alias(\"max_salary\")\n",
    ")\n",
    "dept_stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec0e7f7",
   "metadata": {},
   "source": [
    "## 4. Joins (Very Important for Interviews)\n",
    "\n",
    "Joins are one of the most commonly asked topics in PySpark interviews. Understanding different types of joins and their performance implications is crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781a3d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTERMEDIATE-ADVANCED: All Types of Joins\n",
    "\n",
    "# Create additional DataFrames for join examples\n",
    "departments_data = [\n",
    "    (\"Engineering\", \"Tech\", \"Building A\"),\n",
    "    (\"Marketing\", \"Business\", \"Building B\"),\n",
    "    (\"HR\", \"Support\", \"Building C\"),\n",
    "    (\"Finance\", \"Business\", \"Building B\")\n",
    "]\n",
    "\n",
    "dept_schema = StructType([\n",
    "    StructField(\"department\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True)\n",
    "])\n",
    "\n",
    "departments_df = spark.createDataFrame(departments_data, dept_schema)\n",
    "\n",
    "print(\"=== Original DataFrames ===\")\n",
    "print(\"Employees:\")\n",
    "df.show()\n",
    "print(\"Departments:\")\n",
    "departments_df.show()\n",
    "\n",
    "# 1. INNER JOIN (most common)\n",
    "print(\"\\n=== INNER JOIN ===\")\n",
    "inner_join_result = df.join(departments_df, \"department\", \"inner\")\n",
    "inner_join_result.select(\"name\", \"department\", \"salary\", \"category\", \"location\").show()\n",
    "\n",
    "# 2. LEFT JOIN (LEFT OUTER)\n",
    "print(\"\\n=== LEFT JOIN ===\")\n",
    "left_join_result = df.join(departments_df, \"department\", \"left\")\n",
    "left_join_result.select(\"name\", \"department\", \"salary\", \"category\", \"location\").show()\n",
    "\n",
    "# 3. RIGHT JOIN (RIGHT OUTER)\n",
    "print(\"\\n=== RIGHT JOIN ===\")\n",
    "right_join_result = df.join(departments_df, \"department\", \"right\")\n",
    "right_join_result.select(\"name\", \"department\", \"salary\", \"category\", \"location\").show()\n",
    "\n",
    "# 4. FULL OUTER JOIN\n",
    "print(\"\\n=== FULL OUTER JOIN ===\")\n",
    "full_join_result = df.join(departments_df, \"department\", \"outer\")\n",
    "full_join_result.select(\"name\", \"department\", \"salary\", \"category\", \"location\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8fb0d3",
   "metadata": {},
   "source": [
    "## 5. Window Functions (Advanced - Frequently Asked in Interviews)\n",
    "\n",
    "Window functions are very powerful and commonly asked in advanced PySpark interviews. They allow you to perform calculations across a set of rows related to the current row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b1697c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADVANCED: Window Functions Examples\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead, first, last\n",
    "\n",
    "# Create dataset for window functions\n",
    "sales_data = [\n",
    "    (\"Alice\", \"Q1\", 2023, 1000),\n",
    "    (\"Alice\", \"Q2\", 2023, 1200),\n",
    "    (\"Alice\", \"Q3\", 2023, 1100),\n",
    "    (\"Alice\", \"Q4\", 2023, 1300),\n",
    "    (\"Bob\", \"Q1\", 2023, 800),\n",
    "    (\"Bob\", \"Q2\", 2023, 900),\n",
    "    (\"Bob\", \"Q3\", 2023, 950),\n",
    "    (\"Bob\", \"Q4\", 2023, 1000),\n",
    "    (\"Charlie\", \"Q1\", 2023, 1500),\n",
    "    (\"Charlie\", \"Q2\", 2023, 1400),\n",
    "    (\"Charlie\", \"Q3\", 2023, 1600),\n",
    "    (\"Charlie\", \"Q4\", 2023, 1700)\n",
    "]\n",
    "\n",
    "sales_schema = StructType([\n",
    "    StructField(\"salesperson\", StringType(), True),\n",
    "    StructField(\"quarter\", StringType(), True),\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"sales_amount\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "sales_df = spark.createDataFrame(sales_data, sales_schema)\n",
    "sales_df.show()\n",
    "\n",
    "# 1. ROW_NUMBER, RANK, DENSE_RANK\n",
    "print(\"\\n=== Ranking Functions ===\")\n",
    "window_spec = Window.partitionBy(\"salesperson\").orderBy(col(\"sales_amount\").desc())\n",
    "\n",
    "ranked_df = sales_df.withColumn(\"row_number\", row_number().over(window_spec)) \\\n",
    "                   .withColumn(\"rank\", rank().over(window_spec)) \\\n",
    "                   .withColumn(\"dense_rank\", dense_rank().over(window_spec))\n",
    "ranked_df.show()\n",
    "\n",
    "# 2. LAG and LEAD functions\n",
    "print(\"\\n=== LAG and LEAD Functions ===\")\n",
    "time_window = Window.partitionBy(\"salesperson\").orderBy(\"quarter\")\n",
    "\n",
    "lag_lead_df = sales_df.withColumn(\"previous_quarter_sales\", lag(\"sales_amount\", 1).over(time_window)) \\\n",
    "                     .withColumn(\"next_quarter_sales\", lead(\"sales_amount\", 1).over(time_window)) \\\n",
    "                     .withColumn(\"sales_growth\", \n",
    "                               col(\"sales_amount\") - lag(\"sales_amount\", 1).over(time_window))\n",
    "lag_lead_df.show()\n",
    "\n",
    "# 3. Cumulative operations\n",
    "print(\"\\n=== Cumulative Operations ===\")\n",
    "cumulative_window = Window.partitionBy(\"salesperson\").orderBy(\"quarter\") \\\n",
    "                         .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "cumulative_df = sales_df.withColumn(\"cumulative_sales\", \n",
    "                                   sum(\"sales_amount\").over(cumulative_window)) \\\n",
    "                       .withColumn(\"running_avg\", \n",
    "                                 avg(\"sales_amount\").over(cumulative_window))\n",
    "cumulative_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0803d2",
   "metadata": {},
   "source": [
    "## 6. Common PySpark Interview Questions & Solutions\n",
    "\n",
    "Here are real interview problems that test your PySpark knowledge. Try to solve them before looking at the solutions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e55e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTERVIEW QUESTIONS & SOLUTIONS\n",
    "\n",
    "# Create sample data for interview problems\n",
    "transactions_data = [\n",
    "    (\"2023-01-15\", \"A\", \"Product1\", 100, 2),\n",
    "    (\"2023-01-16\", \"B\", \"Product2\", 150, 1),\n",
    "    (\"2023-01-17\", \"A\", \"Product1\", 100, 3),\n",
    "    (\"2023-01-18\", \"C\", \"Product3\", 200, 1),\n",
    "    (\"2023-01-19\", \"B\", \"Product2\", 150, 2),\n",
    "    (\"2023-01-20\", \"A\", \"Product3\", 200, 1),\n",
    "    (\"2023-02-01\", \"C\", \"Product1\", 100, 4),\n",
    "    (\"2023-02-02\", \"B\", \"Product1\", 100, 1),\n",
    "]\n",
    "\n",
    "transactions_schema = StructType([\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"product\", StringType(), True),\n",
    "    StructField(\"price\", IntegerType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "transactions_df = spark.createDataFrame(transactions_data, transactions_schema)\n",
    "transactions_df = transactions_df.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))\n",
    "transactions_df.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INTERVIEW QUESTION 1: Find the top 3 customers by total purchase amount\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Solution 1\n",
    "top_customers = transactions_df.groupBy(\"customer_id\") \\\n",
    "    .agg(sum(col(\"price\") * col(\"quantity\")).alias(\"total_amount\")) \\\n",
    "    .orderBy(col(\"total_amount\").desc()) \\\n",
    "    .limit(3)\n",
    "top_customers.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INTERVIEW QUESTION 2: Find customers who bought the same product more than once\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Solution 2\n",
    "repeat_customers = transactions_df.groupBy(\"customer_id\", \"product\") \\\n",
    "    .agg(count(\"*\").alias(\"purchase_count\")) \\\n",
    "    .filter(col(\"purchase_count\") > 1) \\\n",
    "    .select(\"customer_id\", \"product\", \"purchase_count\")\n",
    "repeat_customers.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INTERVIEW QUESTION 3: Find the running total of sales for each customer\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Solution 3\n",
    "customer_window = Window.partitionBy(\"customer_id\").orderBy(\"date\") \\\n",
    "                        .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "running_total = transactions_df.withColumn(\"amount\", col(\"price\") * col(\"quantity\")) \\\n",
    "    .withColumn(\"running_total\", sum(\"amount\").over(customer_window)) \\\n",
    "    .select(\"customer_id\", \"date\", \"product\", \"amount\", \"running_total\") \\\n",
    "    .orderBy(\"customer_id\", \"date\")\n",
    "running_total.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INTERVIEW QUESTION 4: Find the second highest salary in each department\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Solution 4: Using our employee data\n",
    "dept_salary_window = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "\n",
    "second_highest = df.withColumn(\"rank\", dense_rank().over(dept_salary_window)) \\\n",
    "                  .filter(col(\"rank\") == 2) \\\n",
    "                  .select(\"department\", \"name\", \"salary\")\n",
    "second_highest.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0501c23b",
   "metadata": {},
   "source": [
    "## 7. Performance Optimization (Advanced Interview Topic)\n",
    "\n",
    "Performance optimization is crucial for PySpark interviews. Understanding how to optimize Spark jobs can set you apart from other candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423c88df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADVANCED: Performance Optimization Techniques\n",
    "\n",
    "# 1. Caching and Persistence\n",
    "print(\"=== Caching Examples ===\")\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "# Cache a DataFrame that will be used multiple times\n",
    "cached_df = df.cache()\n",
    "\n",
    "print(\"Available Storage Levels:\")\n",
    "print(\"MEMORY_ONLY:\", StorageLevel.MEMORY_ONLY)\n",
    "print(\"MEMORY_AND_DISK:\", StorageLevel.MEMORY_AND_DISK)\n",
    "print(\"DISK_ONLY:\", StorageLevel.DISK_ONLY)\n",
    "\n",
    "# Example: Multiple operations on cached data\n",
    "print(\"\\nOperations on cached DataFrame:\")\n",
    "print(\"Count:\", cached_df.count())\n",
    "print(\"Average salary:\", cached_df.agg(avg(\"salary\")).collect()[0][0])\n",
    "print(\"Max salary:\", cached_df.agg(max(\"salary\")).collect()[0][0])\n",
    "\n",
    "# Unpersist when done\n",
    "cached_df.unpersist()\n",
    "\n",
    "# 2. Partitioning\n",
    "print(\"\\n=== Partitioning Examples ===\")\n",
    "\n",
    "# Check current partitions\n",
    "print(f\"Current partitions: {df.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Repartition for better parallelism\n",
    "repartitioned_df = df.repartition(4)\n",
    "print(f\"After repartitioning: {repartitioned_df.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Repartition by column (useful for joins and aggregations)\n",
    "partitioned_by_dept = df.repartition(\"department\")\n",
    "print(\"Partitioned by department\")\n",
    "\n",
    "# 3. Performance Best Practices\n",
    "print(\"\\n=== Performance Best Practices ===\")\n",
    "performance_tips = [\n",
    "    \"1. Use appropriate file formats (Parquet for analytics)\",\n",
    "    \"2. Partition data by frequently filtered columns\",\n",
    "    \"3. Use broadcast joins for small tables (<200MB)\",\n",
    "    \"4. Cache DataFrames used multiple times\",\n",
    "    \"5. Use columnar storage formats\",\n",
    "    \"6. Avoid UDFs when built-in functions are available\",\n",
    "    \"7. Use appropriate number of partitions (2-4x number of cores)\",\n",
    "    \"8. Push down filters as early as possible\",\n",
    "    \"9. Use bucketing for large tables that are frequently joined\",\n",
    "    \"10. Monitor and tune garbage collection\"\n",
    "]\n",
    "\n",
    "for tip in performance_tips:\n",
    "    print(tip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38228443",
   "metadata": {},
   "source": [
    "## 8. Study Plan for PySpark Interviews\n",
    "\n",
    "### Week 1-2: Foundations\n",
    "- Master RDD operations (map, filter, reduce, etc.)\n",
    "- Understand DataFrames and basic operations\n",
    "- Practice creating different types of schemas\n",
    "- Learn file I/O operations (reading/writing different formats)\n",
    "\n",
    "### Week 3-4: Intermediate Concepts\n",
    "- Master all types of joins and their performance implications\n",
    "- Practice aggregations and window functions extensively\n",
    "- Learn SQL operations in PySpark\n",
    "- Understand partitioning and data skewness\n",
    "\n",
    "### Week 5-6: Advanced Topics\n",
    "- Performance optimization techniques\n",
    "- UDFs and when to use them\n",
    "- Streaming concepts (if applicable to your role)\n",
    "- Error handling and debugging\n",
    "\n",
    "### Week 7-8: Interview Preparation\n",
    "- Practice coding problems without looking at solutions first\n",
    "- Time yourself solving problems\n",
    "- Practice explaining your solutions\n",
    "- Review Spark architecture and internals\n",
    "\n",
    "## Key Interview Topics to Master\n",
    "\n",
    "### Technical Questions You Should Be Ready For:\n",
    "1. **Explain Spark Architecture** - Driver, Executors, Cluster Manager\n",
    "2. **RDD vs DataFrame vs Dataset** - When to use which\n",
    "3. **Lazy Evaluation** - How it works and benefits\n",
    "4. **Partitioning Strategies** - Hash vs Range partitioning\n",
    "5. **Join Strategies** - Broadcast, Sort-merge, Hash joins\n",
    "6. **Performance Tuning** - Common bottlenecks and solutions\n",
    "7. **Memory Management** - How Spark manages memory\n",
    "8. **Fault Tolerance** - How Spark handles failures\n",
    "\n",
    "### Coding Patterns to Practice:\n",
    "1. Data cleaning and transformation pipelines\n",
    "2. Complex aggregations with multiple grouping levels\n",
    "3. Window function problems (ranking, running totals, etc.)\n",
    "4. Join optimization scenarios\n",
    "5. Handling null values and data quality issues\n",
    "\n",
    "## Final Tips for Success\n",
    "\n",
    "### During the Interview:\n",
    "- Always explain your thought process\n",
    "- Consider edge cases (null values, empty datasets)\n",
    "- Discuss performance implications of your solutions\n",
    "- Ask clarifying questions about data size and requirements\n",
    "- Be prepared to optimize your initial solution\n",
    "\n",
    "### Code Quality:\n",
    "- Write readable and well-structured code\n",
    "- Use meaningful variable names\n",
    "- Add comments for complex logic\n",
    "- Consider error handling\n",
    "\n",
    "### Remember:\n",
    "- Practice is key - solve problems regularly\n",
    "- Understand the \"why\" behind each solution\n",
    "- Be familiar with Spark UI for debugging\n",
    "- Know when to use which optimization technique\n",
    "\n",
    "Good luck with your PySpark interviews! ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d27154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup - Always close SparkSession when done\n",
    "print(\"Stopping SparkSession...\")\n",
    "spark.stop()\n",
    "print(\"SparkSession stopped successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
